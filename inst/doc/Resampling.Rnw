%\VignetteIndexEntry{Resampling-based inference}
%\VignettePackage{mosaic}
%\VignetteKeywords{mosaic, resampling, bootstrapping, permutation}
\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amsfonts, amssymb, epstopdf, color, url, hyperref} 
\usepackage[margin=1in]{geometry}
\usepackage[parfill]{parskip}
\definecolor{green}{RGB}{0,127,0}
\hypersetup{pdftitle={Resampling-based inference}, colorlinks=true, linkcolor=black, citecolor=black}
\usepackage[bottom]{footmisc}
\usepackage[round]{natbib}
\bibliographystyle{abbrvnat}

\frenchspacing{}

\title{Resampling-based inference using the {\tt mosaic} package}
\author{Daniel Kaplan\thanks{dtkaplan@gmail.com} \\ \footnotesize Macalester College \\
\footnotesize St. Paul, MN
\and Nicholas J. Horton\thanks{nhorton@smith.edu} \\ \footnotesize Smith College \\
\footnotesize Northampton, MA \and 
Randall Pruim\thanks{rpruim@calvin.edu}
\\ \footnotesize Calvin College \\
\footnotesize Grand Rapids, MI
}
\date{\today}
\begin{document}
%\SweaveOpts{concordance=TRUE}

\maketitle

\tableofcontents

<<echo=false>>=
options(continue = "    ")
histogram = function(...){print(lattice::histogram(...))}
xhistogram = function(...){print(mosaic::xhistogram(...))}
@ 

\SweaveOpts{keep.source=TRUE}

\section{Introduction}

This document is intended to describe relatively straightforward ways to undertake a variety of resampling-based inferences through use of the {\tt mosaic} package within R.  
It is derived from a series of problems posed at USCOTS (United
States Conference on Teaching Statistics) 2011 by Robin Lock and colleagues (\url{http://www.causeweb.org/uscots/breakout/breakout3_6.php}).
One of the goals of the {\tt mosaic} package is to provide
elementary commands that can be easily strung together by novices
without having to master the esoteric aspects of programming.  More information about the package and this initiative can be 
found at the Project MOSAIC website: \url{www.mosaic-web.org}.

The {\tt mosaic} operations allow students to implement each of the
operations in what George Cobb calls the ``3 Rs'' of statistical
inference: Randomization, Replication, and Rejection (Cobb, 2007). 
By putting the 3 Rs together in various ways, students learn to
generalize and internalize the logic of inference, rather than just
to blindly follow formulaic methods.

There's an interesting discussion of the role of simulation in Speed (2011), where
he notes the changing role of simulation.  It used to be:
\begin{quote}
something that people did when they can't do the math. $\ldots$ It now seems that we
are heading into an era when all statistical analysis can be done by simulation.
\end{quote}

Arguably, the most important operation in statistics is sampling:
ideally, selecting a random subset from a population.  Regrettably,
sampling takes work and time, so instructors tend to de-emphasize the
actual practice of sampling in favor of theoretical descriptions.
What's more, the algebraic notation in which much of conventional
textbook statistics is written does not offer an obvious notation for sampling.

With the computer, however, these efficiency and notation obstacles
can be overcome.  Sampling can be placed in its rightfully central
place among the statistical concepts in our courses.

Resampling-based inference using permutation testing and bootstrapping are an
increasingly important set of techniques for introductory statistics and beyond.

Bootstrapping and permutation testing are powerful and elegant approaches to estimation and testing, respectively that can be implemented even in many situations
where asymptotic results are
difficult to find or otherwise unsatisfactory (Efron and Tibshirani, 1993; Hesterberg et al 2005).  Bootstrapping involves sampling \emph{with} replacement from a population, repeatedly calculating a sample statistic of interest to empirically construct the sampling distribution. Permutation testing for a two group comparison is done by \emph{permuting} the labels for the grouping variable, then calculating the sample statistic (e.g. difference between two groups using these new labels) to empirically construct the null distribution.

\section{Background and setup}
\subsection{R and RStudio}
\bigskip

R is an open-source statistical environment that has been used at a number of institutions to teach introductory statistics.
Among other advantages, R makes it
easy to demonstrate the concepts of statistical inference through
randomization while providing a sensible path for beginners to
progress to advanced and
professional statistics. RStudio (\url{http://www.rstudio.org}) is an open-source integrated
development environment for R which facilitates use of the system.


\subsection{Setup}
The mosaic package is available over the Internet and can be installed
into R using the standard features of the system (this needs only be done once).  
<<eval=FALSE, echo=TRUE>>=
install.packages("mosaic")
@ 

Once installed, the
package must be loaded so that it is available (this must be done within each R session).  In addition to loading the package, we set the number of digits to display by default as well as the number of simulations to undertake.
<<setup>>=
require(mosaic)
options(digits=3)
numsim <- 250  
@ 

This command would typically be provided in a set-up file for students so that it's executed automatically each time an R session is started.  Next we load two of the datasets that will be used in the examples.

<<eval=TRUE>>=
mustangs = read.csv("MustangPrice.csv")
sleep = read.csv("SleepCaffeine.csv")
@ 

These datasets can also be accessed over the internet using the following {\tt mosaic} commands:
<<eval=FALSE>>=
mustangs = fetchData("MustangPrice.csv")
sleep = fetchData("SleepCaffeine.csv")
@

\section{Resampling in different settings}
\subsection{Bootstrapping a mean (used Mustangs)}

 {\em A student collected data on the selling prices for a sample of used
  Mustang cars being offered for sale at an internet website. The
  price (in \$1,000's), age (in years) and miles driven (in 1,000's)
  for the 25 cars in the sample are given in the [file \texttt{MustangPrice.csv}].  
  Use these data to construct a 90\% confidence interval for the mean
  price (in \$1,000's) of used Mustangs.}

\bigskip

First we start by displaying the distribution of values.
<<dot>>=
stem(mustangs$Price)
@

The mean price can be calculated as
<<meanmust>>=
with(mustangs, mean(Price))
@ 

Even though a single trial is of little use, it's a nice idea to have
students do the calculation to show that they are getting a different
(usually!) result than without resampling.
One resampling trial can be carried out with
<<>>=
with(resample(mustangs), mean(Price))
@
This generates a new sample (with replacement) of the same size as the original one.

Another trial can be carried out with the command:
<<>>=
with(resample(mustangs), mean(Price))
@ 
Let's generate five more:
<<>>=
trials = do(5) * with(resample(mustangs), mean(Price))
trials
@

Now conduct \Sexpr{numsim} resampling trials, saving the results in an object
called \texttt{trials}:
<<>>=
numsim
trials = do(numsim) * with(resample(mustangs), mean(Price))
@ 
This creates a new set of data with the result from each of the {\tt numsim} $=\Sexpr{numsim}$ trials.

Plots of distributions are straightforward, e.g.:
<<hist,fig=true,include=TRUE,pdf=true,height=4,width=4>>=
xhistogram(~ result, data=trials, xlab="Mustang prices (in thousand dollars)")
@ 

Calculation of the 90\% confidence interval can be done directly.
<<qdata>>=
qdata(c(.05, .95), trials$result)
@ 

Alternatively, the standard error from this distribution can be used to estimate the 90\% margin of
error.  First calculate the t (or z) multiplier
for the appropriate degrees of freedom:
<<tstar>>=
tstar = qt(.95, df=24)
zstar = qnorm(.95)
@ 
The resulting
margin of error will be
<<margin>>=
tstar * sd(trials$result)
zstar * sd(trials$result)
@ 


\subsection{Testing a proportion (NFL Overtimes)}

{\em The National Football League (NFL) uses an overtime
period to determine a winner for games that are tied at the end of
regulation time.  The first team to score in the overtime wins the
game and a coin flip is used to determine which team gets the ball
first.  Is there an advantage to winning the coin flip?  Data from the
1974 through 2009 seasons show that the coin flip winner won 240 of
the 428 games where a winner was determined in overtime.  Treat these
as a sample of NFL games to test whether there is sufficient evidence
to show that the proportion of overtime games won by the coin flip
winner is more than one half.}

\bigskip

If the coin flip result were unrelated to the outcome of the game, the
observed 240 game wins out of 428 events would itself be a plausible
outcome of a coin flip.

\paragraph{Style 1} Using the built-in binomial distribution operators.

Generate a simulation where each trial is a random sample  of 428
games from a world in which the null hypothesis holds true.

<<proptable>>=
proptable(rbinom(100000, prob=0.5, size=428) >= 240)
@ 

It's very unlikely, if the null were true, that the coin flip winner
would win 240 or more times.

Of course, such a calculation can be done directly, but that raises
issues such as which tail \texttt{pbinom} is calculating (R always
does the left tail) and adjusting the cut-off appropriately
<<pbinom>>=
pbinom(239, prob=0.5, size=428)
@ 

\paragraph{Style 2} Explicitly simulating a coin flip.

Recognizing that coin flips are a staple of statistics courses, the
\texttt{mosaic} package offers a random flip operator that does the
tabulation for you.  Here is one trial involving flipping 428 coins:
<<coinflip>>=
do(1) * rflip(428)
@ 

We'll do \Sexpr{numsim} trials, and count what fraction of the trials the
winner (say, ``heads'') wins 240 or more:
<<flips1>>=
trials = do(numsim) * rflip(428)
proptable(trials$heads >= 240)
@
<<flips,fig=true,pdf=true,width=4,height=4>>=
xhistogram(~ heads, groups = (heads >= 240), data=trials)
@ 

The observed pattern of 240 wins is not a likely outcome under the null hypothesis.
The shading added through the {\tt groups =} option helps to visually reinforce the result.



\subsection{Permutation test of means from two groups (sleep and memory)} 

 {\em In an experiment on memory, students were given lists of 24 words
  to memorize.  After hearing the words they were assigned at random
  to different groups. One group of 12 students took a nap for 1.5
  hours while a second group of 12 students stayed awake and was given
  a caffeine pill.  The results below display the number of words each
  participant was able to recall after the break.  Test whether the
  data indicate a difference in mean number of words recalled between
  the two treatments.}

\bigskip

<<>>=
sleep
@ 

The Sleep group seems to have remembered somewhat more words on average:
<<obsmean>>=
mean(Words ~ Group, data=sleep)
obs = diff(mean(Words ~ Group, data=sleep))
obs
@ 

<<fig=TRUE,pdf=true,width=4,height=4>>=
bwplot(Words ~ Group, data=sleep)
@

To implement the null hypothesis, scramble the Group with respect to
the outcome, Words:
<<>>=
diff(mean(Words ~ shuffle(Group), data=sleep))
@ 

That's just one trial.  Let's try again:
<<>>=
diff(mean(Words ~ shuffle(Group), data=sleep))
@
To get the distribution under the null
hypothesis, we carry out many trials:

<<sleep,pdf=true,fig=true,width=4,height=4>>=
trials = do(numsim) * diff(mean(Words ~ shuffle(Group), data=sleep))
xhistogram(~ Sleep, groups = Sleep >= obs, data=trials,
  xlab="Distribution of difference in means\nunder the null hypothesis")
@ 

We can calculate the one-sided p-value for this test by summing up the number of 
trials which yielded a result as extreme or more extreme as the observed difference.
Here only \Sexpr{sum(trials$result >= obs)} of the permutations were that large, so 
the p-value is equal to \Sexpr{sum(trials$result >= obs)/numsim}.  We conclude that
it's unlikely that the two groups have the same mean word recall back in their respective 
populations.

\subsection{Permutation test of proportions from two groups (HELP RCT)}

We can undertake a test of difference in two proportions, in this case, the proportion homeless by gender in the HELP randomized clinical trial.

<<>>=
with(HELPrct, table(homeless, sex))
mean(homeless=="housed" ~ sex, data=HELPrct)
obs = diff(mean(homeless=="housed"~ sex, data=HELPrct))
obs  # observed value
@

We will use the same general approach to empirically calculate the null distribution by permuting the labels for the grouping variable.
<<>>=
# compute permutation distribution
nulldist = do(numsim) * diff(mean(homeless=="housed" ~ shuffle(sex), data=HELPrct))
@

<<diffprop,fig=true,pdf=true,width=4,height=4>>=
xhistogram(~ male, groups=(male>=obs), nulldist, xlab="difference in proportions")
@

\subsection{Bootstrapping a correlation}

\noindent
\begin{quotation}
{\em The data on Mustang prices in Problem \#1 also contains the number
 of miles each car had been driven (in thousands).  Find a
 95\% confidence interval for the correlation between price and mileage.}
\end{quotation}

<<>>=
with(mustangs, cor(Price, Miles))
@

<<>>=
trials = do(numsim) * with(resample(mustangs), cor(Price, Miles))
quantiles = qdata(c(.025, .975), trials$result); quantiles
@

<<cor,fig=true,pdf=true,width=4,height=4>>=
xhistogram(~ result, 
  groups = ((result <= quantiles[1]) | (result >= quantiles[2])),
  nbin=30, data=trials)
@ 

\subsection{Bootstrapping a regression model}
But there's no reason to restrict oneself to the correlation: we can also
fit the linear model and consider the coefficients themselves:
<<miles,fig=true,pdf=true,width=4,height=4>>=
do(1) * lm(Price ~ Miles, data=mustangs)
trials = do(numsim) * lm(Price ~ Miles, data=resample(mustangs))
xhistogram(~ Miles, data=trials)
sd(trials) # standard errors
@ 

The predicted average price goes down by $22 \pm 6$ cents per mile driven.


\paragraph{Using Simulations in Other Ways}

The basic technology of resampling and shuffling can be used to
demonstrate many other concepts in statistics than the generation of
confidence intervals and p-values.  For example, it is very useful for
showing the origins of distributions such as t and F.  Similarly, it
can be helpful to show students the distribution of p-values under the
null hypothesis --- students are surprised to see that it's uniform.
Seeing this helps them to understand the sense in which the
``significance level'' refers to a false rejection of the null in a
world in which the null is true.

\section{Acknowledgments}

Thanks to Sarah Anoke for comments on an earlier draft, as well as to Robin
Lock of St. Lawrence University for organizing the session at USCOTS.
Project MOSAIC is supported by the US National Science Foundation (DUE-0920350).
More information about the package and this initiative can be 
found at the Project MOSAIC website: \url{www.mosaic-web.org}.


\section{References}
\begin{itemize}
\item G. W. Cobb, The introductory statistics course: a Ptolemaic curriculum?, 
   \emph{Technology Innovations in Statistics Education}, 2007, 1(1).
\item B. Efron \& R. J. Tibshirani, {\em An introduction to the bootstrap}, 1993, Chapman \& Hall, New York.
\item T Hesterberg, D. S. Moore, S. Monaghan, A. Clipson \& R. Epstein.  {\em Bootstrap methods and permutation tests (2nd edition)}, (2005), W.H. Freeman, New York.
\item D. Kaplan, {\em Statistical modeling: A fresh approach}, \url{http://www.macalester.edu/~kaplan/ISM}.
\item T. Speed, Simulation, \emph{IMS Bulletin}, 2011, 40(3):18.
 

\end{itemize}



\end{document}
